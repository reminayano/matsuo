{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3 精度向上Tips\n",
    "\n",
    "### 3.1 Clippingによる勾配爆発への対処\n",
    "\n",
    "SimpleRNNの課題として、勾配爆発と勾配消失があげられる。\n",
    "\n",
    "このうち、勾配爆発については**Gradient Clipping**とよばれる、勾配の大きさそのものを制限ししまうという手法が有効。\n",
    "\n",
    "![Clipping](https://github.com/reminaya\n",
    "no/matsuo/blob/master/lesson3/figures/Clipping.png?raw=true)\n",
    "\n",
    "この図のグリッド(格子状の線)が損失関数のパラメータに対する振る舞いを表している。勾配は各点でのグリッド面の傾きに対応している。\n",
    "\n",
    "学習にあたっては、損失関数が最小となる点を探したい。こうした急峻な形状の損失関数では、「崖」に相当する部分で勾配爆発が発生する。\n",
    "\n",
    "Clippingを行わない場合、極端な例では上図の左のように、「崖」での勾配爆発で探索点を一気に押し戻され、それまでの探索が無駄になってしまう。\n",
    "\n",
    "そこで、Clippingを行うことで、上図右のように、課題な勾配については制限し、探索が過剰に変化することを抑制している。\n",
    "\n",
    "ただし、あまりに制限値を小さくとると、常に勾配が小さくなり、学習のスピードが落ちる点には注意を要する。\n",
    "\n",
    "kerasでGradient Clippingを行うには、Optimizerを指定する際に引数としてclipnormまたはclipvalueを指定する。\n",
    "\n",
    "```python\n",
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, clipnorm=1.)    # clipnormは勾配の2乗ノルムの最大値を制限する\n",
    "sgd2 = optimizers.SGD(lr=0.01, clipvalue=1.)  # clipvalueは勾配の\"要素\"の絶対値の大きさを制限する\n",
    "# clipnormの方が勾配の方向を変えないという利点があるが、経験的にはどちらの振る舞いも大差ない\n",
    "\n",
    "ada = optimizers.Adagrad(lr=0.01, clipnorm=1.)# SGDに限らずすべてのoptimizerで指定可能\n",
    "\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "```\n",
    "\n",
    "なお、2乗ノルム $||$・$||_2$ とは、\n",
    "- ベクトルに対する、通常の距離(ベクトル$v$に対して $||v||_2 = \\sqrt{v_1^2+v_2^2+\\cdots+v_n^2}$ )\n",
    "\n",
    "- 行列に対する、Frobeniusノルム(行列$A$に対して $||A||_2 = \\sqrt{\\Sigma_{i=1}^m \\Sigma_{j=1}^n a_{ij}^2 }$)\n",
    "\n",
    "のことを表す。\n",
    "\n",
    "### 3.2 ショートカットショートカットとゲートによる勾配消失への対応\n",
    "\n",
    "Clippingは勾配爆発への対処として有力であるが、勾配消失への対処としては**ショートカット**とよばれる手法が効果的。\n",
    "\n",
    "これは、各層の出力にその層への入力だったものも加えてしまうという手法。\n",
    "\n",
    "(RNNに限らず、一般に第l層目の出力を $o^{(l)} = f^{(l)}(o^{(l-1)})+o^{(l-1)}$ とすることで可能)\n",
    "\n",
    "![shortcut](https://github.com/reminayano/matsuo/blob/master/lesson3/figures/shortcut.png?raw=true)\n",
    "\n",
    "一見、これが勾配消失に有効か疑わしいが、このショートカットにより、この層の勾配が **「１＋元の勾配」** と増加する。\n",
    "\n",
    "そのため、勾配の積が積み重なる、入り口に近い層でも勾配が消失することなく、パラメータの更新が可能となる。\n",
    "\n",
    "-----\n",
    "また、このショートカットの概念に加え、**ゲート**とい概念も重要。\n",
    "\n",
    "これは、ショートカットの一般化として重み付き和を考えるもの。\n",
    "\n",
    "(つまり、$f^{(l)}(o^{(l-1)})$ と $o^{(l-1)}$ に各々係数を掛けたうえで足し合わせる。)\n",
    "\n",
    "![gate](https://github.com/reminayano/matsuo/blob/master/lesson3/figures/gate.png?raw=true)\n",
    "\n",
    "なお図中の $\\odot$ は要素ごとの積(アダマール積)を表す。\n",
    "\n",
    "この**ゲートの係数も学習**することにより、前の層から\n",
    "の情報と現在の層による情報の重みを最適に調整できる。\n",
    "\n",
    "つまり以前の層からの情報の忘却度合いをちょうどよく決められます。\n",
    "\n",
    "----\n",
    "これまでのテクニックはRNNに限らず、一般のDNNの層を重ねる際に使用できる。\n",
    "\n",
    "RNNの場合は、特にこのゲートを(層方向だけでなく)時間方向に適用することで長期記憶や短期記憶を実現した、**ゲート付きRNN**が重要となる。\n",
    "\n",
    "次節以降では、このゲート付きRNNとして代表的なLSTMとGRUについて触れる。\n",
    "\n",
    "### 3.4 LSTM\n",
    "\n",
    "**Long Short Term Memory(LSTM)** はゲートの考え方を時間方向の隠れ状態の計算に用いることで、系列内の長期的な相互依存性をモデル化できるようにしたRNNで、頻繁に使用されるモデルの1つ。\n",
    "\n",
    "下図にLSTMの時点tでのモデルの模式図を示す。(このような素子が横に系列長Tだけ並んでいる)\n",
    "\n",
    "![lstm](https://github.com/reminayano/matsuo/blob/master/lesson3/figures/lstm.png?raw=true)\n",
    "\n",
    "LSTMでは時点tでの入力情報(時点tでの入力$x_t$、前時点での出力$h_{t-1}$ を結合したもの)を入力ゲート$i_t$ を介して取り込んでいる。\n",
    "\n",
    "この入力情報を用いて、前時点までの長期的な系列情報 $c_{t-1}$ を$c_t$ に更新する。\n",
    "\n",
    "なお、このとき前時点までの情報$c_{t-1}$ をどれだけ重視するか、忘却ゲート$f_t$ が制御している。\n",
    "\n",
    "最終的な出力は、時点tまでの系列情報$c_t$ を出力ゲート$o_t$ によって調整することで決定される。\n",
    "\n",
    "これらを線形変換や活性化関数を含めて数式で表現すると以下のようになる。\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\bar{h}_t & = & \\tanh(W_\\bar{h}x_t+R_\\bar{h}h_{t-1}+b_\\bar{h}) \\\\\n",
    "c_t & = & i_t \\odot \\bar{h}_t+f_t \\odot c_{t-1} \\\\\n",
    "h_t & = & o_t\\odot \\tanh (c_t)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "そて、各ゲートの係数 $i_t、f_t、o_t$ は以下の様にして、時点tの入力情報を元に決定される。\n",
    "\n",
    "パラメータ $W、R、b$ を学習により決定することで、ゲート係数も多様なデータに柔軟に対応できる。\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "i_t & = & \\sigma(W_ix_t+R_ih_{t-1}+b_i) \\\\\n",
    "f_t & = & \\sigma(W_fx_t+R_fh_{t-1}+b_f) \\\\\n",
    "o_t & = & \\sigma(W_ox_t+R_oh_{t-1}+b_o) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "ここまで詳しくLSTMの構造を見てきたが、kerasにおいてはそうした詳細を気にする必要がほとんどない。keras.layers.LSTMを用いるだけで利用できるので。\n",
    "\n",
    "大事なのは、RNNのなかっでも長い系列に強い(系列内の長期的な相互依存性をモデル可能)モデルであるという特性を認識しておくこと。\n",
    "\n",
    "引数\n",
    "- units : ユニット数(系列長T)\n",
    "- activation : 活性化関数\n",
    "- recurrect_activation : ゲート係数の計算で使用する活性化関数\n",
    "- use_bias : バイアスベクトル( $Wx_t+Rh_{t-1}$ に付け加えるベクトル)を使用するか\n",
    "- {kernel, recurrent, bias}_initializer : 各パラメータの初期化法(kernelはW、reccurentはRを指す)\n",
    "- unit_forget_bias : 忘却ゲートを1に初期化\n",
    "- {kernel, recurrent, bias, activity}_regularizer : 各パラメータの正規化(activityは出力=activationを指す)\n",
    "- {kernel, recurrrent, bias}_constraint : 各パラメータに課す制約\n",
    "- dropout : 入力についてのdropoutの比率(Wに対するdropout)\n",
    "- recurrent_dropout : 再帰についてのdropoutの比率(Rに対するdropout)\n",
    "- return_sequences : Falseなら出力としては系列の最後の出力のみ($o_T$のみ)を返す、Trueなら出力として完全な出力($o_1, o_2, \\cdots, o_T$)を返す\n",
    "- return_state : Trueのときは出力とともに最後の状態($S_T$)を返す\n",
    "- go_backwards : Trueのときは入力系列を後ろから処理する(出力も逆順に)\n",
    "- stateful : Trueのときは、前バッチの各サンプルに対する最後の状態を、次のバッチのサンプルに対する初期状態として引き継ぐ\n",
    "- unroll : (高速化のためのオプション) Trueのときは再帰が展開され高速化されるが、よりメモリに負荷がかかる。(短い系列にのみ適する)\n",
    "\n",
    "```python\n",
    "keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,\n",
    "                  kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                  unit_forget_bias=True,\n",
    "                  kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                  kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
    "                  dropout=0.0, recurrent_dropout=0.0, implementation=1,\n",
    "                  return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "```\n",
    "\n",
    "### 3.3 GRU\n",
    "\n",
    "**Gated Recurrent Unit(GRU)** は、ゲートの考え方を利用しながら、隠れ状態ベクトル$h_t$のみに長期の情報も集約したモデル。\n",
    "\n",
    "(LSTMでは、長期の状態と短期の状態を$c_t, h_t$ の2本で管理していた)\n",
    "\n",
    "![gru](https://github.com/reminayano/matsuo/blob/master/lesson3/figures/gru.png?raw=true)\n",
    "\n",
    "このように、状態ベクトルは$h_t$のみ、ゲートも$r_t, z_t$ の2つのみで、LSTMと比較して(同じ系列長なら)計算量も使用空間量も少なく済む。\n",
    "\n",
    "(ただし、RNNとしての性能については、タスクごとにLSTMとGRUのどちらが有意となるかは違っていて、甲乙つけがたい)\n",
    "\n",
    "GRUの実装もkerasでは簡単に行うことができ、次のkeras.layers.GRUを用いる。\n",
    "\n",
    "主な引数\n",
    "- units : ユニット数(系列長T)\n",
    "- activation : 活性化関数\n",
    "- recurrect_activation : 内部で使用する活性化関数\n",
    "- use_bias : バイアスベクトル( $Ux_t+Wh_{t-1}$ に付け加えるベクトル)を使用するか\n",
    "- {kernel, recurrent, bias}_initializer : 各パラメータの初期化法(kernelはW、reccurentはRを指す)\n",
    "- unit_forget_bias : 忘却ゲートを1に初期化\n",
    "- {kernel, recurrent, bias, activity}_regularizer : 各パラメータの正規化(activityは出力=activationを指す)\n",
    "- {kernel, recurrrent, bias}_constraint : 各パラメータに課す制約\n",
    "- dropout : 入力についてのdropoutの比率(Wに対するdropout)\n",
    "- recurrent_dropout : 再帰についてのdropoutの比率(Rに対するdropout)\n",
    "- return_sequences : Falseなら出力としては系列の最後の出力のみ($o_T$のみ)を返す、Trueなら出力として完全な出力($o_1, o_2, \\cdots, o_T$)を返す\n",
    "- return_state : Trueのときは出力とともに最後の状態($S_T$)を返す\n",
    "- go_backwards : Trueのときは入力系列を後ろから処理する(出力も逆順に)\n",
    "- stateful : Trueのときは、前バッチの各サンプルに対する最後の状態を、次のバッチのサンプルに対する初期状態として引き継ぐ\n",
    "- unroll : (高速化のためのオプション) Trueのときは再帰が展開され高速化されるが、よりメモリに負荷がかかる。(短い系列にのみ適する)\n",
    "\n",
    "```python\n",
    "keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                 kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                 kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
    "                 dropout=0.0, recurrent_dropout=0.0, implementation=1,\n",
    "                 return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
